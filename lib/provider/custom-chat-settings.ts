// https://docs.mistral.ai/platform/endpoints/
export type CustomChatModelId =
  | 'open-mistral-7b'
  | 'open-mixtral-8x7b'
  | 'open-mixtral-8x22b'
  | 'mistral-small-latest'
  | 'mistral-medium-latest'
  | 'mistral-large-latest'
  | (string & {})

export interface CustomChatSettings {
  /**
Whether to inject a safety prompt before all conversations.

Defaults to `false`.
   */
  safePrompt?: boolean
}

/**
A tool has a name, a description, and a set of parameters.

Note: this is **not** the user-facing tool definition. The AI SDK methods will
map the user-facing tool definitions to this format.
 */
type LanguageModelV1FunctionTool = {
  /**
  The type of the tool. Only functions for now, but this gives us room to
  add more specific tool types in the future and use a discriminated union.
     */
  type: 'function'
  /**
  The name of the tool. Unique within this model call.
     */
  name: string
  description?: string
}

declare class ToolCallParseError extends Error {
  readonly cause: unknown
  readonly text: string
  readonly tools: LanguageModelV1FunctionTool[]
  constructor({
    cause,
    text,
    tools,
    message
  }: {
    cause: unknown
    text: string
    tools: LanguageModelV1FunctionTool[]
    message?: string
  })
  static isToolCallParseError(error: unknown): error is ToolCallParseError
  toJSON(): {
    name: string
    message: string
    stack: string | undefined
    cause: unknown
    text: string
    tools: LanguageModelV1FunctionTool[]
  }
}

declare class TypeValidationError extends Error {
  readonly value: unknown
  readonly cause: unknown
  constructor({ value, cause }: { value: unknown; cause: unknown })
  static isTypeValidationError(error: unknown): error is TypeValidationError
  toJSON(): {
    name: string
    message: string
    cause: unknown
    stack: string | undefined
    value: unknown
  }
}

declare class UnsupportedFunctionalityError extends Error {
  readonly functionality: string
  constructor({ functionality }: { functionality: string })
  static isUnsupportedFunctionalityError(
    error: unknown
  ): error is UnsupportedFunctionalityError
  toJSON(): {
    name: string
    message: string
    stack: string | undefined
    functionality: string
  }
}

declare class UnsupportedJSONSchemaError extends Error {
  readonly reason: string
  readonly schema: unknown
  constructor({
    schema,
    reason,
    message
  }: {
    schema: unknown
    reason: string
    message?: string
  })
  static isUnsupportedJSONSchemaError(
    error: unknown
  ): error is UnsupportedJSONSchemaError
  toJSON(): {
    name: string
    message: string
    stack: string | undefined
    reason: string
    schema: unknown
  }
}

type LanguageModelV1CallSettings = {
  /**
   * Maximum number of tokens to generate.
   */
  maxTokens?: number
  /**
   * Temperature setting.
   *
   * It is recommended to set either `temperature` or `topP`, but not both.
   */
  temperature?: number
  /**
   * Nucleus sampling.
   *
   * It is recommended to set either `temperature` or `topP`, but not both.
   */
  topP?: number
  /**
   * Presence penalty setting. It affects the likelihood of the model to
   * repeat information that is already in the prompt.
   */
  presencePenalty?: number
  /**
   * Frequency penalty setting. It affects the likelihood of the model
   * to repeatedly use the same words or phrases.
   */
  frequencyPenalty?: number
  /**
   * The seed (integer) to use for random sampling. If set and supported
   * by the model, calls will generate deterministic results.
   */
  seed?: number
  /**
   * Abort signal for cancelling the operation.
   */
  abortSignal?: AbortSignal
}

/**
A prompt is a list of messages.

Note: Not all models and prompt formats support multi-modal inputs and
tool calls. The validation happens at runtime.

Note: This is not a user-facing prompt. The AI SDK methods will map the
user-facing prompt types such as chat or instruction prompts to this format.
 */
type LanguageModelV1Prompt = Array<LanguageModelV1Message>
type LanguageModelV1Message =
  | {
      role: 'system'
      content: string
    }
  | {
      role: 'user'
      content: Array<LanguageModelV1TextPart | LanguageModelV1ImagePart>
    }
  | {
      role: 'assistant'
      content: Array<LanguageModelV1TextPart | LanguageModelV1ToolCallPart>
    }
  | {
      role: 'tool'
      content: Array<LanguageModelV1ToolResultPart>
    }
/**
Text content part of a prompt. It contains a string of text.
 */
interface LanguageModelV1TextPart {
  type: 'text'
  /**
  The text content.
     */
  text: string
}
/**
Image content part of a prompt. It contains an image.
 */
interface LanguageModelV1ImagePart {
  type: 'image'
  /**
  Image data as a Uint8Array (e.g. from a Blob or Buffer) or a URL.
     */
  image: Uint8Array | URL
  /**
  Optional mime type of the image.
     */
  mimeType?: string
}
/**
Tool call content part of a prompt. It contains a tool call (usually generated by the AI model).
 */
interface LanguageModelV1ToolCallPart {
  type: 'tool-call'
  /**
  ID of the tool call. This ID is used to match the tool call with the tool result.
   */
  toolCallId: string
  /**
  Name of the tool that is being called.
   */
  toolName: string
  /**
  Arguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.
     */
  args: unknown
}
/**
Tool result content part of a prompt. It contains the result of the tool call with the matching ID.
 */
interface LanguageModelV1ToolResultPart {
  type: 'tool-result'
  /**
  ID of the tool call that this result is associated with.
   */
  toolCallId: string
  /**
  Name of the tool that generated this result.
    */
  toolName: string
  /**
  Result of the tool call. This is a JSON-serializable object.
     */
  result: unknown
  /**
  Optional flag if the result is an error or an error message.
     */
  isError?: boolean
}

type LanguageModelV1CallOptions = LanguageModelV1CallSettings & {
  /**
   * Whether the user provided the input as messages or as
   * a prompt. This can help guide non-chat models in the
   * expansion, bc different expansions can be needed for
   * chat/non-chat use cases.
   */
  inputFormat: 'messages' | 'prompt'
  /**
   * The mode affects the behavior of the language model. It is required to
   * support provider-independent streaming and generation of structured objects.
   * The model can take this information and e.g. configure json mode, the correct
   * low level grammar, etc. It can also be used to optimize the efficiency of the
   * streaming, e.g. tool-delta stream parts are only needed in the
   * object-tool mode.
   */
  mode:
    | {
        type: 'regular'
        tools?: Array<LanguageModelV1FunctionTool>
      }
    | {
        type: 'object-json'
      }
    | {
        type: 'object-tool'
        tool: LanguageModelV1FunctionTool
      }
  /**
   * A language mode prompt is a standardized prompt type.
   *
   * Note: This is **not** the user-facing prompt. The AI SDK methods will map the
   * user-facing prompt types such as chat or instruction prompts to this format.
   * That approach allows us to evolve the user  facing prompts without breaking
   * the language model interface.
   */
  prompt: LanguageModelV1Prompt
}

/**
 * Warning from the model provider for this call. The call will proceed, but e.g.
 * some settings might not be supported, which can lead to suboptimal results.
 */
type LanguageModelV1CallWarning =
  | {
      type: 'unsupported-setting'
      setting: keyof LanguageModelV1CallSettings
    }
  | {
      type: 'other'
      message: string
    }

/**
Reason why a language model finished generating a response.

Can be one of the following:
- `stop`: model generated stop sequence
- `length`: model generated maximum number of tokens
- `content-filter`: content filter violation stopped the model
- `tool-calls`: model triggered tool calls
- `error`: model stopped because of an error
- `other`: model stopped for other reasons
 */
type LanguageModelV1FinishReason =
  | 'stop'
  | 'length'
  | 'content-filter'
  | 'tool-calls'
  | 'error'
  | 'other'

type LanguageModelV1FunctionToolCall = {
  toolCallType: 'function'
  toolCallId: string
  toolName: string
  /**
   * Stringified JSON object with the tool call arguments. Must match the
   * parameters schema of the tool.
   */
  args: string
}

/**
Log probabilities for each token and its top log probabilities.
 */
type LanguageModelV1LogProbs = Array<{
  token: string
  logprob: number
  topLogprobs: Array<{
    token: string
    logprob: number
  }>
}>

/**
 * Experimental: Specification for a language model that implements the language model
 * interface version 1.
 */
type LanguageModelV1 = {
  /**
   * The language model must specify which language model interface
   * version it implements. This will allow us to evolve the language
   * model interface and retain backwards compatibility. The different
   * implementation versions can be handled as a discriminated union
   * on our side.
   */
  readonly specificationVersion: 'v1'
  /**
   * Name of the provider for logging purposes.
   */
  readonly provider: string
  /**
   * Provider-specific model ID for logging purposes.
   */
  readonly modelId: string
  /**
   * Default object generation mode that should be used with this model when
   * no mode is specified. Should be the mode with the best results for this
   * model. `undefined` can be returned if object generation is not supported.
   *
   * This is needed to generate the best objects possible w/o requiring the
   * user to explicitly specify the object generation mode.
   */
  readonly defaultObjectGenerationMode: 'json' | 'tool' | 'grammar' | undefined
  /**
   * Generates a language model output (non-streaming).
   *
   * Naming: "do" prefix to prevent accidental direct usage of the method
   * by the user.
   */
  doGenerate(options: LanguageModelV1CallOptions): PromiseLike<{
    /**
     * Text that the model has generated. Can be undefined if the model
     * has only generated tool calls.
     */
    text?: string
    /**
     * Tool calls that the model has generated. Can be undefined if the
     * model has only generated text.
     */
    toolCalls?: Array<LanguageModelV1FunctionToolCall>
    /**
     * Finish reason.
     */
    finishReason: LanguageModelV1FinishReason
    /**
     * Usage information.
     */
    usage: {
      promptTokens: number
      completionTokens: number
    }
    /**
     * Raw prompt and setting information for observability provider integration.
     */
    rawCall: {
      /**
       * Raw prompt after expansion and conversion to the format that the
       * provider uses to send the information to their API.
       */
      rawPrompt: unknown
      /**
       * Raw settings that are used for the API call. Includes provider-specific
       * settings.
       */
      rawSettings: Record<string, unknown>
    }
    /**
     * Optional raw response information for debugging purposes.
     */
    rawResponse?: {
      /**
       * Response headers.
       */
      headers?: Record<string, string>
    }
    warnings?: LanguageModelV1CallWarning[]
    /**
     * Logprobs for the completion.
     * `undefined` if the mode does not support logprobs or if was not enabled
     */
    logprobs?: LanguageModelV1LogProbs
  }>
  /**
   * Generates a language model output (streaming).
   *
   * Naming: "do" prefix to prevent accidental direct usage of the method
   * by the user.
   *
   * @return A stream of higher-level language model output parts.
   */
  doStream(options: LanguageModelV1CallOptions): PromiseLike<{
    stream: ReadableStream<LanguageModelV1StreamPart>
    /**
     * Raw prompt and setting information for observability provider integration.
     */
    rawCall: {
      /**
       * Raw prompt after expansion and conversion to the format that the
       * provider uses to send the information to their API.
       */
      rawPrompt: unknown
      /**
       * Raw settings that are used for the API call. Includes provider-specific
       * settings.
       */
      rawSettings: Record<string, unknown>
    }
    /**
     * Optional raw response data.
     */
    rawResponse?: {
      /**
       * Response headers.
       */
      headers?: Record<string, string>
    }
    warnings?: LanguageModelV1CallWarning[]
  }>
}
type LanguageModelV1StreamPart =
  | {
      type: 'text-delta'
      textDelta: string
    }
  | ({
      type: 'tool-call'
    } & LanguageModelV1FunctionToolCall)
  | {
      type: 'tool-call-delta'
      toolCallType: 'function'
      toolCallId: string
      toolName: string
      argsTextDelta: string
    }
  | {
      type: 'finish'
      finishReason: LanguageModelV1FinishReason
      logprobs?: LanguageModelV1LogProbs
      usage: {
        promptTokens: number
        completionTokens: number
      }
    }
  | {
      type: 'error'
      error: unknown
    }
type LanguageModelV1ResponseMetadata = {}

export {
  type LanguageModelV1,
  type LanguageModelV1CallOptions,
  type LanguageModelV1CallWarning,
  type LanguageModelV1FinishReason,
  type LanguageModelV1FunctionTool,
  type LanguageModelV1FunctionToolCall,
  type LanguageModelV1ImagePart,
  type LanguageModelV1LogProbs,
  type LanguageModelV1Message,
  type LanguageModelV1Prompt,
  type LanguageModelV1ResponseMetadata,
  type LanguageModelV1StreamPart,
  type LanguageModelV1TextPart,
  type LanguageModelV1ToolCallPart,
  type LanguageModelV1ToolResultPart,
  ToolCallParseError,
  TypeValidationError,
  UnsupportedFunctionalityError,
  UnsupportedJSONSchemaError
}
